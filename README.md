# ğŸ–¼ï¸ CIFAR-10 Image Classification Using Fully Connected Neural Network  
*"From foundational principles to optimized implementation - a complete neural network journey"* ğŸ§   

---

## ğŸ“‹ Project Overview  
In this comprehensive project, we constructed a neural network from scratch using fundamental principles, leveraging the CIFAR-10 dataset for training. The implementation encompassed data preprocessing, forward/backward propagation, and highlighted the transformative power of vectorization for computational efficiency.  

**Core Insight:** This project demonstrates the pivotal role of vectorization in accelerating neural network computations, illuminating its critical importance in modern machine learning workflows. âš¡  

---

## ğŸ‘¨â€ğŸ« Supervision  
Under the guidance of **Prof. Mohammad Mehdi Ebadzadeh**  
ğŸ“… Spring 2022  

---

## ğŸ“š Libraries Used  
- **NumPy** - Fundamental package for scientific computing  
- **Matplotlib** - Comprehensive library for visualization  
- **Scikit-image** - Image processing algorithms  
- **PIL (Pillow)** - Image manipulation capabilities  
- **Glob** - File path pattern matching  
- **OS** - Operating system interface  
- **Time** - Time access and measurement utilities  

---

## ğŸš€ Implementation Steps  

### 1. ğŸ› ï¸ Data Preprocessing  
**Step 1: Data Acquisition & Storage**  
- ğŸ“¥ Read the first 4 classes from CIFAR-10 dataset (airplane âœˆï¸, automobile ğŸš—, bird ğŸ¦, and cat ğŸ±)  
- ğŸ’¾ Store data in matrix format: `(n_samples, width, height, channels)`  
- ğŸ”¢ Encode labels using one-hot representation  

**Steps 2â€“5: Data Transformation Pipeline**  
- ğŸ¶ Grayscale Conversion - Reduce computational complexity by converting RGB to grayscale  
- ğŸ“Š Normalization - Scale pixel values to `[0, 1]` range by dividing by 255  
- ğŸ§© Flattening - Reshape data to `(n_samples, 1024)` for input layer compatibility  
- ğŸ”€ Shuffling - Randomize data order while maintaining data-label correspondence  

---

### 2. ğŸ“ˆ Feedforward Implementation  
Objective: Compute network outputs using forward propagation  

- âœ… Data Selection: 200-sample subset from training data  
- âœ… Parameter Initialization:  
  - ğŸ² Random weight initialization  
  - 0ï¸âƒ£ Zero bias initialization  
- âœ… Output Computation: Matrix multiplication + Sigmoid activation Ïƒ  
- âœ… Model Inference: Class prediction based on maximum activation ğŸ“ˆ  
- âœ… Accuracy Assessment: ~25% baseline accuracy (random chance) ğŸ¯  

**Implementation Note:** Leveraged NumPy for efficient matrix operations  

---

### 3. ğŸ” Backpropagation Implementation  
- Employed backpropagation to iteratively refine model parameters and minimize prediction error  
- âš™ï¸ Hyperparameter Tuning: Careful selection of batch size, learning rate, and epochs  
- ğŸ”§ Algorithm Implementation: Followed standard pseudo-code for parameter updates  

**ğŸ“Š Performance Metrics:**  
- Model accuracy on 200-sample subset  
- Execution time measurement â±ï¸  
- Expected performance: ~30% accuracy (accounting for random initialization)  
- ğŸ“‰ Cost Visualization: Plotted average cost reduction per epoch  

---

### 4. âš¡ Vectorization Optimization  
Implemented vectorized operations to dramatically improve computational efficiency  

- ğŸ¯ Feedforward Vectorization: Matrix-based implementation  
- ğŸ”„ Backpropagation Vectorization: Eliminated iterative loops  

**ğŸ“ˆ Enhanced Evaluation:**  
- Increased to 20 epochs for comprehensive assessment  
- Reported final model accuracy and execution time  
- Multiple executions to account for performance variability  
- Cost trajectory visualization over training  

---

### 5. ğŸ§ª Model Testing & Evaluation  
Comprehensive performance assessment using full dataset (4 classes, 8000 samples)  

**ğŸ‹ï¸ Training Configuration:** Optimized hyperparameters  

**ğŸ“‹ Evaluation Metrics:**  
- Training set accuracy ğŸ“Š  
- Test set accuracy ğŸ“‰  
- Comparative performance analysis  
- Learning visualization: Average cost reduction over epochs  

---

## ğŸ¯ Key Achievements  
- âœ… Built neural network from foundational principles  
- âœ… Implemented efficient data preprocessing pipeline  
- âœ… Demonstrated dramatic performance improvement through vectorization  
- âœ… Achieved measurable accuracy on CIFAR-10 subset  
- âœ… Visualized learning process through cost reduction graphs  

---

## ğŸ“ Repository Structure  
```bash
ğŸ“¦ CIFAR10-NeuralNetwork
â”œâ”€â”€ ğŸ“„ README.md # Project documentation
â”œâ”€â”€ ğŸ“Š data/ # Dataset handling utilities
â”œâ”€â”€ ğŸ§  models/ # Neural network implementation
â”œâ”€â”€ ğŸ“ˆ results/ # Performance metrics and visualizations
â”œâ”€â”€ ğŸ”¬ experiments/ # Testing and evaluation scripts
â””â”€â”€ ğŸ“œ requirements.txt # Project dependencies
```
## ğŸš€ How to Run  

**1. Install dependencies:**  
```bash
pip install -r requirements.txt
```
**2. Preprocess data:**
``` bash
python data/preprocessing.py
```

**3. Train model:**
``` bash
python models/train.py
```

**4. Evaluate performance:**
```bash
python experiments/evaluate.py
```
## ğŸ“Š Expected Results

* Baseline Accuracy: ~25â€“30% (random initialization)

* Vectorized Speedup: 5â€“10x performance improvement

* Final Accuracy: Measurable improvement over baseline

* Learning Curve: Consistent cost reduction across epochs

## ğŸ”® Future Enhancements

* Additional layer architectures

* Alternative activation functions (ReLU, tanh)

* Regularization techniques (Dropout, L2)

* Hyperparameter optimization framework

* Extension to full CIFAR-10 dataset (10 classes)
